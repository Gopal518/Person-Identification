{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdaa191",
   "metadata": {},
   "source": [
    "# custom loss function in tensorﬂow/keras to train a model for binary classiﬁcation and apply l2 reguralisation on only convolution and dense layers. Only the custom loss function is required as solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830622d",
   "metadata": {},
   "source": [
    "Loss functions, also called error functions, assess how close a model's predictions are to the actual target values. A lower loss indicates better model performance by showing that the model's predictions are closer to the desired target values. \n",
    "Below are some Loss function use for the Binary Class Classification\n",
    "\n",
    "### 1. Squared hinge loss: \n",
    "Squared hinge loss is a function tailored for \"maximum margin\" binary classification tasks. It smoothens the error function's surface, simplifying numerical computations. For proper use, the target variable should be adjusted to belong to the set of {-1, 1}. \n",
    "It is recommended to utilize this loss function alongside the tanh() activation function in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    # Modify target labels to be in the set of {-1, 1}\n",
    "    y_true = 2 * y_true - 1\n",
    "    \n",
    "    # Calculate Hinge Loss\n",
    "    loss = tf.maximum(0., 1. - y_true * y_pred)\n",
    "    \n",
    "    return loss, tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d50ec",
   "metadata": {},
   "source": [
    "### 2. Cross-Entropy/Logistic Loss (CE)\n",
    "Cross-entropy loss, also called logistic loss, is frequently used in binary classification tasks with classes 0 and 1. It measures the difference between the actual class and the predicted value, usually ranging from 0 to 1. For best results.\n",
    "it's recommended to use the sigmoid activation function as the final layer before applying cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sigmoid_cross_entropy_loss'''\n",
    "def sigmoid_cross_entropy_loss(y_true, y_pred):\n",
    "    # Apply sigmoid function to predicted values\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    \n",
    "    # Clip values to avoid extremes of the log function\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # Compute the binary cross-entropy loss\n",
    "    term_0 = y_true * tf.math.log(y_pred)\n",
    "    term_1 = (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "    loss = -(term_0 + term_1)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''weighted_binary_crossentropy'''\n",
    "def weighted_binary_crossentropy(y_true, y_pred, pos_weight):\n",
    "    epsilon = 1e-7  # Small constant to avoid log(0) or log(1)\n",
    "    \n",
    "    # Clip predicted values to avoid log(0) or log(1)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Calculate binary cross-entropy loss with weights for positive class\n",
    "    loss = - (pos_weight * y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "'''Example usage with different weights'''\n",
    "pos_weight_1 = 0.5\n",
    "pos_weight_2 = 0.75\n",
    "\n",
    "# Calculate weighted binary cross-entropy losses\n",
    "weighted_loss_1 = weighted_binary_crossentropy(y_true, y_pred, pos_weight_1)\n",
    "weighted_loss_2 = weighted_binary_crossentropy(y_true, y_pred, pos_weight_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fbf15",
   "metadata": {},
   "source": [
    "## Custom loss function in tensorﬂow to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b809a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''custom loss function in tensorﬂow/keras to train a model for binary classiﬁcation'''\n",
    "\n",
    "def custom_loss_with_l2_reg(l2=0.01):\n",
    "    # Binary cross-entropy loss function\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        epsilon = 1e-7  # Small constant to avoid log(0) or log(1)\n",
    "        \n",
    "        # Clip values to avoid log(0) or log(1)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Calculate binary cross-entropy loss\n",
    "        binary_loss = - (y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        binary_loss = tf.reduce_mean(binary_loss)\n",
    "        \n",
    "        # Calculate L2 regularization term\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "        \n",
    "        # Total loss with L2 regularization\n",
    "        total_loss = binary_loss + l2 * l2_loss\n",
    "        return total_loss\n",
    "    \n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "'''apply l2 reguralisation on only convolution and dense layers'''\n",
    "\n",
    "# Create a Sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=custom_loss_with_l2_reg(l2=0.01), metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9ec44",
   "metadata": {},
   "source": [
    "#### Assessment:\n",
    "1. Advantages:\n",
    "Integrates binary cross-entropy for classification with L2 regularization in a single loss function.\n",
    "Offers control over model complexity to avoid overfitting.\n",
    "\n",
    "2. Considerations:\n",
    "The l2 parameter controls the strength of regularization and needs tuning based on dataset complexity.\n",
    "Effective for preventing overfitting while optimizing for binary classification.\n",
    "3. Usage:\n",
    "Suitable for binary classification tasks requiring simultaneous optimization of cross-entropy and model regularization.\n",
    "4. Flexibility:\n",
    "Provides flexibility to adjust the regularization strength for finding the right balance between fitting the data and controlling model complexity.\n",
    "\n",
    "This combined loss function is useful for binary classification models, offering control over complexity while optimizing for accurate classification. Fine-tuning the regularization strength is crucial for optimal performance based on dataset characteristics and model complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
